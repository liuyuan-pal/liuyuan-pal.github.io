
<!DOCTYPE html>
<html>
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <title>Yuan Liu - Homepage</title>
      <link rel="stylesheet" href="./source_files/main.css">

   </head>
   <!-- Tab links -->
   <body style="" data-new-gr-c-s-check-loaded="14.977.0">
      <header>
         <div id="header">
            <div class="tab">
               <!-- <div class="tab-name"><a style="text-decoration: none;" href="#">Cheng Lin</a></div> -->
               <div class="tab-link"><a style="text-decoration: none;" href="#mentor">Mentoring</a></div>
               <div class="tab-link"><a style="text-decoration: none;" href="#publications">Publications</a></div>
               <div class="tab-link"><a style="text-decoration: none;" href="#news">Openings</a></div>
               <!-- <div class="tab-link"><a style="text-decoration: none;" href="#">Home</a></div> -->
            </div>
         </div>
         <script>
            $("#header").load("./header.html");
         </script>
      </header>
      
	  
	  <div class="content top_block">
         <table class="about-table">
            <tbody>
               <tr>
                  <td class="image-column">
                     <img src="./images/selfie-new-v2.jpg" alt="Photo" width="220" >
                  </td>
                  <td class="description-column">
                     <div class="content text">
                        <h3>Yuan Liu (刘缘)</h3>
						      <h4>Incoming Assistant Professor@HKUST, PostDoc@NTU, PhD@HKU </h4>
                        <p>yuanly@ust.hk</p>
                  <p style="text-align: justify">
                     I'm visiting NTU as a PostDoc working with Prof. <a href="https://liuziwei7.github.io/">Ziwei Liu</a>. 
                     I obtained my PhD degree at HKU advised by Prof. <a href="https://www.cs.hku.hk/people/academic-staff/wenping">Wenping Wang</a> and Prof. <a href="https://homepages.inf.ed.ac.uk/tkomura/">Taku Komura</a>. 
                     I'm one of the organizers of <a href="https://anysyn3d.github.io/about.html">AnySyn3D</a>. My research mainly concentrates on 3D vision and graphics. 
                     I currently work on topics about 3D AIGC including neural rendering, neural representations, and 3D generative models.
                  </p>
                  
                  <!-- In 2018-2019, I worked in CAD&amp;CG lab of Zhejiang University advised by Prof. <a href="https://xzhou.me/">Xiaowei Zhou</a>.  -->
                  <!-- I received my master degree at LIESMARS of Wuhan University advised by Prof. <a href="http://www.lmars.whu.edu.cn/enbisheng-yang/">Bisheng Yang</a>.  -->
                  <!-- My research interest includes computer graphics and 3D computer vision. I currently work on topics about 3D AIGC including neural rendering/representations and diffusion generative models.  -->
                  <!-- <p style="color:red; font-weight:bold;">I'll graduate before Jul. 2024 and I'm looking for a research position on CG and 3D CV (Diffusion/NeRF/NeuS/GS)!</p> -->
                        <p>
                        </p>
						<p class = "news">
						
						</p>
						
						
						<a target=_blank href="https://github.com/liuyuan-pal" target="_blank">[Github]</a> &nbsp
						<a target=_blank href="https://twitter.com/YuanLiu41955461" target="_blank">[Twitter]</a> &nbsp
						<a target=_blank href="https://scholar.google.com/citations?user=yRAHVcgAAAAJ" target="_blank">[Google Scholar]</a>
						



                     </div>
                  </td>
               </tr>
            </tbody>
         </table>
      </div>
	  
      <div class="content">
         <a name="news"></a>
         <h3>Openings</h3>
         <hr>
         <p>
         I will join <a href="https://isd.hkust.edu.hk/">ISD HKUST</a> as an assistant professor in 2025 spring, leading the <b>Intelligent Graphics Lab</b>. 
         <b style="color:red"> 
            We’re always on the lookout for passionate PhD and visiting students from across the globe to join our research team! 
            We especially welcome international students with a strong drive and a solid background in related fields. 
            <!-- If you’re eager to make an impact in cutting-edge research, we’d love to have you on board! -->
         </b>
          <!-- <b style="color:red">Currently, There are no positions for PhD students (2025 fall) and RA, but there are still openings for visiting students. </b> -->
         </p>

         <p>
         We are currently working on the following topics.
         </p>
         <ol>
            <li>3D representations (like NeuS, NeRF, 3D GS).</li>
            <li>3D asset generation.</li>
            <li>3D-aware video generation.</li>
         </ol>
         <p >
         Some recent works can be found in the <a href="#publications">Publications</a> section.
         If you are interested in working with me on these topics, please fill the Google form 
         <a href="https://docs.google.com/forms/d/e/1FAIpQLSehvKvgl7eJc8n8qonDb5tBwkMd4h5zQYIDKxmsD5X-IiybTQ/viewform?usp=sf_link">here</a> 
         and drop an email to <a href="mailto:yuanly@connect.hku.hk">yuanly@ust.hk</a>.
         I have experience in mentoring master or junior PhD students. Refer to the <a href="#mentor">Mentoring</a> section for more details.
         </p>

         <p>
            <b>Join Us at HKUST!</b>
            Are you passionate about 3D vision and graphics? At HKUST, one of Asia's top universities, you'll have the opportunity to engage in groundbreaking research alongside world-class faculty in a vibrant, innovative environment.
            We warmly welcome students from all over the world to join our diverse and inclusive community.
         </p>

         <p>
            We offer a competitive monthly stipend of 19,700 HKD (around 2,500 USD) to support your academic journey. Exceptional candidates are also encouraged to apply for the prestigious <a href="https://fytgs.hkust.edu.hk/scholarships/hong-kong-phd-fellowship-scheme">HKPFS</a>, which provides an enhanced stipend of 28,000 HKD (approximately 3,600 USD).
            With these scholarships, you’ll enjoy an excellent quality of life in Hong Kong while advancing your research and shaping the future of 3D technology. We can't wait to welcome you to our thriving research community!
         </p>

         <a name="publications"></a>
         <h3>Selected Publications (<a href="https://scholar.google.com/citations?user=yRAHVcgAAAAJ">Google Scholar</a>)</h3>
		   <hr>
         <!-- <p> Full publication list can be found <a href="full-pub.html">here</a>.</p> -->
        <b>*Equal contribution †Corresponding author</b>
         <table class="publication-table">
         
         
         <tr>
            <td class="image-column">
               <div class="demo-video">
                  <video width=100% height=100% muted autoplay loop>
                     <source src="images/align3r.mp4" type="video/mp4">
                     </video>
               </div>
            </td>
            <td class="description-column">
                  <div class="paper_title">Align3R: Aligned Monocular Depth Estimation for Dynamic Videos</div>
                  <div class="paper_authors">
                     Jiahao Lu*, Tianyu Huang*, Peng Li, Zhiyang Dou, Cheng Lin, Zhiming Cui, Zhen Dong, Sai-Kit Yeung, Wenping Wang, <b>Yuan Liu</b>†
                  </div>
                  <div class="paper_venue"> <b>arXiv 2024</b> </div>
               <div class="paper_materials">
               <span class="link"><a target=_blank href="https://arxiv.org/abs/2412.03079">[Paper]</a></span>
               <span class="link"><a target=_blank href="https://igl-hkust.github.io/Align3R.github.io/">[Project Page]</a></span>
               <span class="link"><a target=_blank href="https://github.com/jiah-cloud/Align3R">[Code]</a></span>
            </div>
            </td>
            </tr>
         
         <tr>
            <td class="image-column">
               <div class="demo-video">
                  <video width=100% height=100% muted autoplay loop>
                     <source src="images/vistadream.mp4" type="video/mp4">
                     </video>
               </div>
            </td>
            <td class="description-column">
                  <div class="paper_title">VistaDream: Sampling multiview consistent images for single-view scene reconstruction</div>
                  <div class="paper_authors">
                     Haiping Wang, <b>Yuan Liu</b>†, Ziwei Liu, Wenping Wang, Zhen Dong†, Bisheng Yang
                  </div>
                  <div class="paper_venue"> <b>arXiv 2024</b> </div>
               <div class="paper_materials">
               <span class="link"><a target=_blank href="https://arxiv.org/abs/2410.16892">[Paper]</a></span>
               <span class="link"><a target=_blank href="https://vistadream-project-page.github.io/">[Project Page]</a></span>
               <span class="link"><a target=_blank href="https://github.com/WHU-USI3DV/VistaDream">[Code]</a></span>
            </div>
            </td>
            </tr>

         <tr>
            <td class="image-column">
               <div class="demo-video"> <video width=100% height=100% muted autoplay loop>
               <source src="images/era3d-lr.mp4" type="video/mp4">
               </video></div>
            </td>
            <td class="description-column">
                  <div class="paper_title">Era3D: High-Resolution Multiview Diffusion using Efficient Row-wise Attention</div>
                  <div class="paper_authors">
                        Peng Li*, <b>Yuan Liu*</b>, Xiaoxiao Long†, Feihu Zhang, Cheng Lin, Mengfei Li, Xingqun Qi, Shanghang Zhang, Wei Xue, Wenhan Luo, Ping Tan, Wenping Wang, Qifeng Liu, Yike Guo†
                  </div>
                  <div class="paper_venue"> <b>NeurIPS 2024</b> </div>
               <div class="paper_materials">
               <span class="link"><a target=_blank href="https://arxiv.org/abs/2405.11616">[Paper]</a></span>
               <span class="link"><a target=_blank href="https://penghtyx.github.io/Era3D/">[Project Page]</a></span>
               <span class="link"><a target=_blank href="https://github.com/pengHTYX/Era3D">[Code]</a></span>
            </div>
            </td>
            </tr>

         <tr>
            <td class="image-column">
               <div class="demo-video"> <video width=100% height=100% muted autoplay loop>
               <source src="images/syncdreamer-lr.mp4" type="video/mp4">
               </video></div>
            </td>
            <td class="description-column">
                  <div class="paper_title">SyncDreamer: Generating Multiview-consistent Images from a Single-view Image</div>
                  <div class="paper_authors">
                     <b>Yuan Liu*</b>, Cheng Lin*, Zijiao Zeng, Xiaoxiao Long†, Lingjie Liu, Taku Komura, Wenping Wang†
                  </div>
                  <div class="paper_venue"> <b>ICLR 2024 (Spotlight)</b> </div>
               <div class="paper_materials">
               <span class="link"><a target=_blank href="https://arxiv.org/abs/2309.03453">[Paper]</a></span>
               <span class="link"><a target=_blank href="https://liuyuan-pal.github.io/SyncDreamer/">[Project Page]</a></span>
               <span class="link"><a target=_blank href="https://github.com/liuyuan-pal/SyncDreamer">[Code]</a></span>
            </div>
            </td>
            </tr>

         <tr>
            <td class="image-column">
               <div class="demo-video"> <video width=100% height=100% muted autoplay loop>
               <source src="images/wonder3d-lr.mp4" type="video/mp4">
               </video></div>
            </td>
            <td class="description-column">
                  <div class="paper_title">Wonder3D: Single Image to 3D using Cross-Domain Diffusion</div>
                  <div class="paper_authors">
                  Xiaoxiao Long*, Yuan-Chen Guo*, Cheng Lin†, <b>Yuan Liu</b>, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, Wenping Wang†
                  </div>
                  <div class="paper_venue"> <b>CVPR 2024 (Highlight)</b> </div>
               <div class="paper_materials">
               <span class="link"><a target=_blank href="https://arxiv.org/pdf/2310.15008.pdf">[Paper]</a></span>
               <span class="link"><a target=_blank href="https://www.xxlong.site/Wonder3D/">[Project Page]</a></span>
               <span class="link"><a target=_blank href="https://github.com/xxlong0/Wonder3D">[Code]</a></span>
            </div>
            </td>
            </tr>

         <tr>
            <td class="image-column">
               <div class="demo-video"> <video width=100% height=100% muted autoplay loop>
               <source src="images/nero-lr.mp4" type="video/mp4">
               </video></div>
            </td>
            <td class="description-column">
                  <div class="paper_title">NeRO: Neural Geometry and BRDF Reconstruction of Reflective Objects from Multiview Images</div>
                  <div class="paper_authors">
                  <b>Yuan Liu</b></a>, Peng Wang, Cheng Lin, Xiaoxiao Long, Jiepeng Wang, Lingjie Liu, Taku Komura, Wenping Wang
                  </div>
                  <div class="paper_venue"> <b>SIGGRAPH 2023 (TOG)</b> </div>
               <div class="paper_materials">
               <span class="link"><a target=_blank href="https://arxiv.org/abs/2305.17398">[Paper]</a></span>
               <span class="link"><a target=_blank href="https://liuyuan-pal.github.io/NeRO/">[Project Page]</a></span>
               <span class="link"><a target=_blank href="https://github.com/liuyuan-pal/NeRO">[Code]</a></span>
            </div>
            </td>
            </tr>
         
         <tr>
            <td class="image-column">
               <div class="demo-video"> <video width=100% height=100% muted autoplay loop>
               <source src="images/f2nerf-lr.mp4" type="video/mp4">
               </video></div>
            </td>
            <td class="description-column">
                  <div class="paper_title">F<sup>2</sup>NeRF: Fast Neural Radiance Field Training with Free Camera Trajectories</div>
                  <div class="paper_authors">
                  Peng Wang*, <b>Yuan Liu*</b>, Zhaoxi Chen, Ziwei Liu, Lingjie Liu, Taku Komura, Christian Theobalt, Wenping Wang
                  </div>
                  <div class="paper_venue"> <b>CVPR 2023 (Highlight)</b> </div>
               <div class="paper_materials">
               <span class="link"><a target=_blank href="https://arxiv.org/abs/2303.15951">[Paper]</a></span>
               <span class="link"><a target=_blank href="https://totoro97.github.io/projects/f2-nerf/">[Project Page]</a></span>
               <span class="link"><a target=_blank href="https://github.com/Totoro97/f2-nerf">[Code]</a></span>
            </div>
            </td>
            </tr>			
			
			<tr>
               <td class="image-column">
				  <div class="demo-video"> <video width=100% height=100% muted autoplay loop>
					<source src="images/neuray-lr.mp4" type="video/mp4">
				  </video></div>
               </td>
               <td class="description-column">
                  <div class="paper_title">Neural Rays for Occlusion-aware Image-based Rendering</div>
                  <div class="paper_authors">
                     <b>Yuan Liu</b>, Sida Peng, Lingjie Liu, Qianqian Wang, Peng Wang, Christian Theobalt, Xiaowei Zhou, Wenping Wang		  
                  </div>
                  <div class="paper_venue"> <b>CVPR 2022</b> </div>
				  <div class="paper_materials">
                  <span class="link"><a target=_blank href="https://arxiv.org/abs/2107.13421">[Paper]</a></span>
				  <span class="link"><a target=_blank href="https://liuyuan-pal.github.io/NeuRay/">[Project Page]</a></span>
                  <span class="link"><a target=_blank href="https://github.com/liuyuan-pal/NeuRay">[Code]</a></span>

				  </div>
               </td>
            </tr>
			
			
			<tr>
               <td class="image-column">
				  <div class="demo-video"> <video width=100% height=100% muted autoplay loop>
					<source src="images/neus.mp4" type="video/mp4">
				  </video></div>
               </td>
               <td class="description-column">
                  <div class="paper_title">NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction</div>
                  <div class="paper_authors">
                     Peng Wang, Lingjie Liu, <b>Yuan Liu</b>, Christian Theobalt, Taku Komura, Wenping Wang		  
                  </div>
                  <div class="paper_venue"> <b>NeurIPS 2021 (Spotlight)</b> </div>
				  <div class="paper_materials">
                  <span class="link"><a target=_blank href="https://arxiv.org/abs/2106.10689">[Paper]</a></span>
				  <span class="link"><a target=_blank href="https://lingjie0206.github.io/papers/NeuS/">[Project Page]</a></span>
                  <span class="link"><a target=_blank href="https://github.com/Totoro97/NeuS">[Code]</a></span>

				  </div>
               </td>
            </tr>
			
         </table>



         <a name="mentor"></a>
         <h3>Mentoring</h3>
         <hr>
         I spent wonderful time working with talented master or junior PhD students on some interesting research projects as follows.
         
   
         <ul>
            <li>
            <a href="https://hpwang-whu.github.io/">Haiping Wang</a>: 
            <strong>YOHO</strong> (<a href="https://hpwang-whu.github.io/YOHO/">MM'22</a>), 
            <strong>RoReg</strong> (<a href="https://hpwang-whu.github.io/RoReg/">TPAMI'23</a>)
            <strong>SGHR</strong> (<a href="https://github.com/WHU-USI3DV/SGHR">CVPR'23</a>)
            <strong>FreeReg</strong> (<a href="https://whu-usi3dv.github.io/FreeReg/">ICLR'23</a>)
            </li>
            <ul>
               <li>PhD at WHU.</li>
            </ul>
            <li>
            <a href="https://runsong123.github.io/">Runsong Zhu</a>: 
            <strong>AdaFit</strong> (<a href="https://runsong123.github.io/AdaFit/">ICCV'21</a>)
            </li>
            <ul><li>Master at WHU, Ph.D. at CUHK</li></ul>
            <li>
            <a href="https://zzzyuqing.github.io/">Yuqing Zhang</a>: 
            <strong>DreamMat</strong> (<a href="https://zzzyuqing.github.io/dreammat.github.io/">SIGGRAPH'24</a>)
            </li>
            <ul><li>Master at ZJU</li></ul>
            <li>
               <a href="https://shanemankiw.github.io/">Jionghao Wang</a>:
               <strong>SO-SMPL</strong> (<a href="https://shanemankiw.github.io/SO-SMPL/">ECCV'24</a>)
               <ul><li>Master at SJTU, PhD at TAMU</li></ul>
            </li>
            <li>
            <a href="https://scholar.google.com/citations?user=8eTLCkwAAAAJ&hl=zh-CN">Peng Li</a>: 
            <strong>Era3D</strong> (<a href="https://penghtyx.github.io/Era3D/">NeurIPS'24</a>)
            <ul><li>PhD at HKUST</li></ul>
            </li>
            <li>
            <a href="https://github.com/Asparagus15">Wenqi Jiangying</a>: 
            <strong>GaussianShader</strong> (<a href="https://asparagus15.github.io/GaussianShader.github.io/">CVPR'24</a>)
            <ul><li>PhD at ShanghaiTech University</li></ul>
            </li>
            <li>
            <a href="https://github.com/liuar0512">Anran Liu</a>: 
            <strong>NeRFBuff</strong> (<a href="https://ieeexplore.ieee.org/abstract/document/10508469/">TVCG'24</a>),
            <strong>Part123</strong> (<a href="https://liuar0512.github.io/part123_official_page/">SIGGRAPH'24</a>)
            <ul><li>PhD at HKU</li></ul>
            </li>
   
         </ul>

      </div>
	  
      <!-- End of the publication -->

      <!-- <div class="content"> -->
		 <!-- <a name="experiences"></a> -->
         <!-- <h3>Experiences</h3> -->
		  <!-- <hr> -->
         <!-- <table class="publication-table"> -->
               <!-- <td class="general-item-left"> -->
                  <!-- <h5></h5> -->
               <!-- </td> -->
			   
               <!-- <td class="general-item-right"> -->
				 <!-- <div class="content text"><li><a class="affliation", target=_blank href="https://niessnerlab.org/index.html">Visual Computing Group</a>,  -->
				 <!-- Technical University of Munich (TUM) <br> -->
				 <!-- <b>Research Visit</b> <br> -->
				 <!-- Advisor: <a  target=_blank href="https://niessnerlab.org/index.html">Prof. Matthias Nießner</a> <br> -->
				 <!-- May 2019 - Oct 2019 -->
				 <!-- </div> -->
				 <!-- <br> -->
				 
				 <!-- <div class="content text"><li> <a class="affliation", target=_blank href="http://ercdm.sdu.edu.cn/">Research Center of Digital Media Technology</a>, Ministry of Education <br>  -->
				 <!-- <b>Research Assistant</b> <br> -->
				 <!-- Advisor: <a  target=_blank href="https://www.sc.sdu.edu.cn/info/1045/1738.htm">Prof. Chenglei Yang</a> <br>  -->
				 <!-- Dec 2014 - June 2016 -->
				 <!-- </div> -->
               <!-- </td> -->
         <!-- </table> -->
      <!-- </div> -->



	
	<div class="content">
	<br>
	<br>
			<hr>
			<div align="center">
				<b>&copy;Yuan Liu.</b>&nbsp Last updated: 9th Dec., 2024.
			</div>
			<br>
	<br>
	</div>


   </body>
</html>
